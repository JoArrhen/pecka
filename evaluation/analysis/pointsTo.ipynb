{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def pointsto_df(directory, project, best=False):\n",
    "    file_path = f\"../results/{directory}/{project}\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return None, None\n",
    "    df = pd.read_csv(file_path, comment='#')\n",
    "\n",
    "    # Convert time from nanoseconds to seconds\n",
    "    df['time'] = df['time'] / 1000000000\n",
    "    # Normalize time within each runid\n",
    "    df['normalized_time'] = df.groupby('runid')['time'].transform(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0)\n",
    "\n",
    "\n",
    "    if best:\n",
    "        df_best_values = df.loc[df.groupby('runid')['time'].idxmin()].reset_index()\n",
    "    else:\n",
    "        df_last_values = df.groupby('runid').last().reset_index()\n",
    "\n",
    "    df_to_use = df_best_values if best else df_last_values\n",
    "\n",
    "    sample_count = df['sample_count'].iloc[0]\n",
    "\n",
    "\n",
    "    # Aggregate mean values for various metrics\n",
    "    df_mean_times = df_to_use.groupby(['distance'])['time'].mean().reset_index(name='mean_time')\n",
    "    df_mean_allocs = df_to_use.groupby(['distance'])['alloc_count'].mean().reset_index(name='mean_alloc_count')\n",
    "    df_mean_types = df_to_use.groupby(['distance'])['type_count'].mean().reset_index(name='mean_type_count')\n",
    "\n",
    "    # Combine the mean dataframes\n",
    "    df_mean = pd.merge(df_mean_times, df_mean_allocs, on='distance', how='left')\n",
    "    df_mean = pd.merge(df_mean, df_mean_types, on='distance', how='left')\n",
    "    #df_mean = pd.merge(df_mean, df_max_memory, on='distance', how='left')\n",
    "\n",
    "    # Adjust mean values by the sample count\n",
    "    df_mean['mean_time'] /= sample_count\n",
    "    df_mean['mean_alloc_count'] /= sample_count\n",
    "    df_mean['mean_type_count'] /= sample_count\n",
    "    #df_mean['max_memory_usage'] /= 10**9 # convert from bytes to Gb\n",
    "\n",
    "    # Add proportions of types/allocs/time compared to the highest value\n",
    "    max_dist = df_mean['distance'].max()\n",
    "    max_allocs = df_mean.loc[df_mean['distance'] == max_dist, 'mean_alloc_count'].values[0]\n",
    "    max_types = df_mean.loc[df_mean['distance'] == max_dist, 'mean_type_count'].values[0]\n",
    "    max_time = df_mean.loc[df_mean['distance'] == max_dist, 'mean_time'].values[0]\n",
    "\n",
    "    df_mean['percent_types'] = (df_mean['mean_type_count'] / max_types) * 100\n",
    "    df_mean['percent_allocs'] = (df_mean['mean_alloc_count'] / max_allocs) * 100\n",
    "    df_mean['percent_time'] = (df_mean['mean_time'] / max_time) * 100\n",
    "    \n",
    "    return df, df_mean\n",
    "\n",
    "df, df_mean = pointsto_df(\"sde-oberon_pointsto_3_3_20240603130030\", \"antlr-2.7.2_bfpa.new\")\n",
    "df_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_df(directory, project):\n",
    "    file_path = f\"../results/{directory}/{project}\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    df = pd.read_csv(file_path, comment='#', header=None, names=['distance', 'memory', 'pass'])\n",
    "    df = df[df['pass'] == 1].groupby('distance')['memory'].min().reset_index(name='min_memory')\n",
    "    return df\n",
    "\n",
    "df_memory = memory_df(\"sde-oberon_memory_1_1_20240514114444\", \"antlr-2.7.2_bfpa.new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_df(directory, project, best=False):\n",
    "    file_path = f\"../results/{directory}/{project}\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return None, None\n",
    "    df = pd.read_csv(file_path, comment='#')\n",
    "\n",
    "    # Convert time from nanoseconds to seconds\n",
    "    df['time'] = df['time'] / 1000000000\n",
    "    # Normalize time within each runid\n",
    "    df['normalized_time'] = df.groupby('runid')['time'].transform(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0)\n",
    "\n",
    "    if best:\n",
    "        df_best_values = df.loc[df.groupby('runid')['time'].idxmin()].reset_index()\n",
    "    else:\n",
    "        df_last_values = df.groupby('runid').last().reset_index()\n",
    "\n",
    "    df_to_use = df_best_values if best else df_last_values\n",
    "    \n",
    "    mean = df_to_use['time'].mean()\n",
    "\n",
    "    return df, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_project_data(pointsto_dir, memory_dir, parse_dir):\n",
    "    project_names = [\"antlr-2.7.2\", \"commons-cli\", \"commons-jxpath\", \"jackson-core\", \"jackson-dataformat-xml\"]\n",
    "    project_names = [\"commons-jxpath\", \"antlr-2.7.2\", \"weka\", \"struts\", \"castor-1.3.3\", \"fop-0.95\", \"pmd-4.2.5\", \"jfreechart-1.0.0\", \"joda-time\"]\n",
    "    results = []\n",
    "    \n",
    "    for project_name in project_names:\n",
    "        df, df_mean = pointsto_df(pointsto_dir, f\"{project_name}_bfpa.new\", best=False)\n",
    "        df_memory = memory_df(memory_dir, f\"{project_name}_bfpa.new\")\n",
    "        df_parse, parse_mean = parse_df(parse_dir, f\"{project_name}_bfpa.new\", best=False)\n",
    "        if df_memory is not None:\n",
    "            df_mean = pd.merge(df_mean, df_memory, on='distance', how='left').fillna(-1)\n",
    "        if df is not None and df_mean is not None:\n",
    "            result = {\n",
    "                \"name\": project_name,\n",
    "                \"data\": df,\n",
    "                \"means\": df_mean,\n",
    "                \"parse_mean\": parse_mean\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "memory = \"sde-oberon_memory_1_1_20240522122042\"\n",
    "parse = \"sde-oberon_compile_50_10_20240516135401\"\n",
    "oberon_res = collect_project_data(\"sde-oberon_pointsto_3_3_20240517121924\", memory, parse)\n",
    "\n",
    "\n",
    "def create_avg(result_list):\n",
    "    return pd.concat([res['means'] for res in result_list]).groupby('distance').mean().reset_index() \n",
    "\n",
    "create_avg(oberon_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def inspect(df, title=\"\"):\n",
    "    rows=2\n",
    "    cols=2\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=('Time by Index for All Runs', 'Normalized Time by Index for All Runs', 'Time by Iteration for Each ID', 'Normalized Time by Iteration for Each ID'))\n",
    "\n",
    "    def next_pos(prev_row, prev_col):\n",
    "        if prev_col < cols:\n",
    "            return prev_row, prev_col + 1\n",
    "        else:\n",
    "            return prev_row + 1, 1\n",
    "\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "\n",
    "    unique_runids = df['runid'].unique()\n",
    "    runid_to_index = {runid: index for index, runid in enumerate(unique_runids)}\n",
    "    num_colors = len(unique_runids)\n",
    "\n",
    "    colors = ['rgba(' + ','.join([str(int(255 * x)) for x in cmap(i / num_colors)[:3]]) + ',1.0)' for i in range(num_colors)]\n",
    "    random.shuffle(colors)\n",
    "\n",
    "    row = col = 1\n",
    "    for runid, group in df.groupby('runid'):\n",
    "        color_index = runid_to_index[runid]\n",
    "        fig.add_trace(go.Scatter(x=group.index, y=group['time'], mode='lines', name=f'ID {runid}', line=dict(color=colors[color_index])), row=row, col=col)\n",
    "        fig.update_xaxes(title_text=\"Index\", row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"Time\", row=row, col=col)\n",
    "\n",
    "    row, col = next_pos(row, col)\n",
    "    for runid, group in df.groupby('runid'):\n",
    "        color_index = runid_to_index[runid]\n",
    "        fig.add_trace(go.Scatter(x=group.index, y=group['normalized_time'], mode='lines', name=f'ID {runid}', line=dict(color=colors[color_index])), row=row, col=col)\n",
    "        fig.update_xaxes(title_text=\"Index\", row=row, col=col, matches='x')\n",
    "        fig.update_yaxes(title_text=\"Normalized time\", row=row, col=col)\n",
    "\n",
    "    row, col = next_pos(row, col)\n",
    "    for runid, group in df.groupby('runid'):\n",
    "        color_index = runid_to_index[runid]\n",
    "        fig.add_trace(go.Scatter(x=group['iteration'], y=group['time'], mode='lines', name=f'ID {runid}', line=dict(color=colors[color_index])), row=row, col=col)\n",
    "        fig.update_xaxes(title_text=\"iteration\", row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"time\", row=row, col=col)\n",
    "\n",
    "    row, col = next_pos(row, col)\n",
    "    for runid, group in df.groupby('runid'):\n",
    "        color_index = runid_to_index[runid]\n",
    "        fig.add_trace(go.Scatter(x=group['iteration'], y=group['normalized_time'], mode='lines', name=f'ID {runid}', line=dict(color=colors[color_index])), row=row, col=col)\n",
    "        fig.update_xaxes(title_text=\"iteration\", row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"Normalized Time\", row=row, col=col)\n",
    "\n",
    "\n",
    "    fig_witdh=500\n",
    "    fig.update_layout(height=fig_witdh*rows, width=fig_witdh*cols, title_text=title, showlegend=True)\n",
    "    fig.show()\n",
    "    \n",
    "#inspect(steady_state[1]['data'], \"antlr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot_titles = ['Distance vs Mean Time', 'Distance vs Alloc Count', 'Distance vs Type Count', 'Distance vs Memory Usage']\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "dash_styles = ['solid', 'dot', 'dash', 'longdash', 'dashdot', 'longdashdot']\n",
    "\n",
    "\n",
    "def plot_metrics(results_dfs, average_df=None):\n",
    "    metrics = ['mean_time', 'mean_alloc_count', 'mean_type_count', 'min_memory']\n",
    "    file_names = ['mean_time_evaluation.pdf', 'alloc_count_evaluation.pdf', 'type_count_evaluation.pdf', 'memory_usage_evaluation.pdf']\n",
    "\n",
    "    # Ensure the figures directory exists\n",
    "    if not os.path.exists('figures'):\n",
    "        os.makedirs('figures')\n",
    "\n",
    "    for metric, title, file_name in zip(metrics, subplot_titles, file_names):\n",
    "        fig = go.Figure()\n",
    "        if average_df is not None:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=average_df['distance'], \n",
    "                y=average_df[metric], \n",
    "                mode='lines+markers',\n",
    "                name='All Projects',\n",
    "                opacity=0.5,\n",
    "                line=dict(color='gray', dash='solid', width=2),\n",
    "                marker=dict(symbol='diamond', size=8)\n",
    "            ))\n",
    "\n",
    "        for i, res in enumerate(results_dfs):\n",
    "            df = res[\"means\"]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=df['distance'], \n",
    "                y=df[metric], \n",
    "                mode='lines+markers',\n",
    "                name=res[\"name\"],\n",
    "                line=dict(color=colors[i % len(colors)], dash=dash_styles[i % len(dash_styles)])\n",
    "            ))\n",
    "        \n",
    "        if metric == \"mean_time\":\n",
    "            yaxis_title = \"Mean Time (s)\"\n",
    "        elif metric == \"min_memory\":\n",
    "            yaxis_title = \"Memory Usage (MB)\"\n",
    "        else:\n",
    "            yaxis_title = metric.replace('_', ' ').capitalize()\n",
    "\n",
    "        fig.update_layout(\n",
    "            #title=title,\n",
    "            xaxis_title=\"Distance\",\n",
    "            yaxis_title=yaxis_title,\n",
    "            showlegend=True,\n",
    "            margin=dict(l=0, r=0, t=0, b=100),\n",
    "            legend=dict(\n",
    "                x=0.5,  # Center the legend\n",
    "                y=-0.15,  # Position the legend below the x-axis\n",
    "                xanchor='center',  # Anchor the legend's center at x position\n",
    "                orientation='h',  # Horizontal orientation of the legend items\n",
    "            ),\n",
    "            font=dict(\n",
    "                size=14\n",
    "            ),\n",
    "            template='plotly_white',\n",
    "        )\n",
    "\n",
    "        fig.write_image(f'figures/{file_name}')\n",
    "\n",
    "plot_metrics(oberon_res, create_avg(oberon_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "def plot_projects_separately(results_df, subplot_titles, colors, dash_styles, show80=False):\n",
    "    if not os.path.exists('figures'):\n",
    "        os.mkdir('figures')\n",
    "\n",
    "    color_cycle = cycle(colors)\n",
    "    dash_cycle = cycle(dash_styles)\n",
    "\n",
    "    for res in results_df:\n",
    "        fig = make_subplots(rows=2, cols=2, subplot_titles=subplot_titles,\n",
    "                            vertical_spacing=0.2, horizontal_spacing=0.15)  # Adjust spacing here\n",
    "        current_color = next(color_cycle)\n",
    "        current_dash = next(dash_cycle)\n",
    "        \n",
    "        # Calculate 80% line value if needed\n",
    "        if show80:\n",
    "            max_type_count = max(res[\"means\"]['mean_type_count'])\n",
    "            eighty_percent_line = 0.8 * max_type_count\n",
    "        \n",
    "        # Distance vs Mean Time\n",
    "        fig.add_trace(go.Scatter(x=res[\"means\"]['distance'], y=res[\"means\"]['mean_time'], mode='lines+markers',\n",
    "                                 name=res[\"name\"], line=dict(color=current_color, dash=current_dash),\n",
    "                                 showlegend=False),\n",
    "                      row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Distance\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Mean Time (s)\", row=1, col=1)\n",
    "        \n",
    "        # Distance vs Alloc Count\n",
    "        fig.add_trace(go.Scatter(x=res[\"means\"]['distance'], y=res[\"means\"]['mean_alloc_count'], mode='lines+markers',\n",
    "                                 line=dict(color=current_color, dash=current_dash),\n",
    "                                 showlegend=False),\n",
    "                      row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"Distance\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Allocations\", row=1, col=2)\n",
    "        \n",
    "        # Distance vs Type Count\n",
    "        trace = go.Scatter(x=res[\"means\"]['distance'], y=res[\"means\"]['mean_type_count'], mode='lines+markers',\n",
    "                           line=dict(color=current_color, dash=current_dash),\n",
    "                           showlegend=False)\n",
    "        fig.add_trace(trace, row=2, col=1)\n",
    "        if show80:\n",
    "            fig.add_trace(go.Scatter(x=res[\"means\"]['distance'], y=[eighty_percent_line]*len(res[\"means\"]['distance']),\n",
    "                                     mode='lines', line=dict(color='red', dash='dot'),\n",
    "                                     name='80% of Max Type Count', showlegend=True),\n",
    "                          row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Distance\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Types\", row=2, col=1)\n",
    "        \n",
    "        # Distance vs Memory Usage\n",
    "        fig.add_trace(go.Scatter(x=res[\"means\"]['distance'], y=res[\"means\"]['min_memory'], mode='lines+markers',\n",
    "                                 line=dict(color=current_color, dash=current_dash),\n",
    "                                 showlegend=False),\n",
    "                      row=2, col=2)\n",
    "        fig.update_xaxes(title_text=\"Distance\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"Memory Usage (MB)\", row=2, col=2)\n",
    "\n",
    "        fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), legend=dict(x=0.5, xanchor='center'))\n",
    "\n",
    "        # Save plot as PDF\n",
    "        pdf_path = f'figures/{res[\"name\"]}_distance_evaluation.pdf'\n",
    "        fig.write_image(pdf_path)\n",
    "        \n",
    "plot_projects_separately(oberon_res, subplot_titles=[], colors=colors, dash_styles=dash_styles, show80=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def gen_latex_singles(results_df):\n",
    "    individual_caption_template = \"{name}\"\n",
    "    whole_figure_caption = \"The metrics measured for each dataset. This is the same graphs as shown in Figure~\\\\ref{fig:mean_time}-\\\\ref{fig:memory_usage}, but plotted separately for each benchmark project.\"\n",
    "    \n",
    "    latex_output = \"\\\\begin{figure}[htbp]\\n\\\\centering\\n\"\n",
    "    for idx, res in enumerate(results_df):\n",
    "        file_path = f'figures/{res[\"name\"]}_distance_evaluation.pdf'\n",
    "        latex_output += f\"\\\\begin{{subfigure}}[b]{{0.45\\\\textwidth}}\\n\"\n",
    "        latex_output += f\"\\\\captionsetup{{margin=0cm}}  % Remove caption margin for this subfigure\\n\"\n",
    "        latex_output += f\"\\\\includegraphics[width=\\\\textwidth]{{{file_path}}}\\n\"\n",
    "        latex_output += f\"\\\\caption{{{individual_caption_template.format(name=res['name'])}}}\\n\"\n",
    "        latex_output += \"\\\\end{subfigure}\\n\"\n",
    "        latex_output += \"\\\\vspace{0.5cm}\\n\"\n",
    "\n",
    "        if idx % 2 == 0:\n",
    "            latex_output += \"\\\\hfill\\n\"\n",
    "        else:\n",
    "            latex_output += \"\\\\\\\\\\n\"\n",
    "\n",
    "    if len(results_df) % 2 != 0:\n",
    "        latex_output = latex_output.rstrip(\"\\\\hfill\\n\")\n",
    "    \n",
    "    latex_output += \"\\\\caption{\" + whole_figure_caption + \"}\\n\"\n",
    "    latex_output += \"\\\\label{fig:singles_result}\\n\"\n",
    "    latex_output += \"\\\\end{figure}\\n\"\n",
    "\n",
    "    if not os.path.exists('generated'):\n",
    "        os.mkdir('generated')\n",
    "    with open('generated/singles_results.tex', 'w') as file:\n",
    "        file.write(latex_output)\n",
    "\n",
    "gen_latex_singles(oberon_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def gen_latex_multi(results_df):\n",
    "    # Configuration variables\n",
    "    captions = [\n",
    "    \"Mean time for calculating the points-to information for a method across the benchmarks for different values of $k$. The gray line shows the mean for all projects. The numbers together with parse data can be seen in Table \\\\ref{tab:parse_times}.\",\n",
    "    \"Mean allocation count for a method across the benchmarks for different values of $k$. The gray line shows the mean for all projects.\",\n",
    "    \"Mean type count for a method across the benchmarks for different values of $k$. They gray line shows the mean for all projects.\",\n",
    "    \"Mean memory usage for calculating the points-to information for a method across the benchmarks for different values of $k$. The gray line shows the mean for all projects.\"\n",
    "    ]\n",
    "\n",
    "    figure_labels = ['fig:mean_time', 'fig:alloc_count', 'fig:type_count', 'fig:memory_usage']\n",
    "    file_names = ['mean_time_evaluation.pdf', 'alloc_count_evaluation.pdf', 'type_count_evaluation.pdf', 'memory_usage_evaluation.pdf']\n",
    "\n",
    "    if not os.path.exists('generated'):\n",
    "        os.mkdir('generated')\n",
    "\n",
    "    latex_output = \"\"\n",
    "\n",
    "    for caption, label, file_name in zip(captions, figure_labels, file_names):\n",
    "        latex_output += \"\\\\begin{figure}[htbp]\\n\\\\centering\\n\"\n",
    "        latex_output += f\"\\\\includegraphics[width=\\\\FigureWidth]{{figures/{file_name}}}\\n\"\n",
    "        latex_output += f\"\\\\caption{{{caption}}}\\n\"\n",
    "        latex_output += f\"\\\\label{{{label}}}\\n\"\n",
    "        latex_output += \"\\\\end{figure}\\n\\n\"\n",
    "\n",
    "    with open('generated/multi_figures.tex', 'w') as file:\n",
    "        file.write(latex_output)\n",
    "\n",
    "gen_latex_multi(oberon_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_culmulative(results_df):\n",
    "    avg = create_avg(results_df)\n",
    "\n",
    "    metrics = ['percent_types', 'percent_time']\n",
    "    desc = ['Types', 'Time']\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    dash_styles = ['solid', 'dot', 'dash', 'longdash', 'dashdot', 'longdashdot']\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i, metric in enumerate(metrics):\n",
    "        fig.add_trace(go.Scatter(\n",
    "                    x=avg['distance'], \n",
    "                    y=avg[metric], \n",
    "                    mode='lines+markers',\n",
    "                    name=desc[i],\n",
    "                    line=dict(color=colors[i % len(colors)], dash=dash_styles[i % len(dash_styles)])\n",
    "                ))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "                xaxis_title=\"Distance\",\n",
    "                yaxis_title=\"Percent\",\n",
    "                margin=dict(l=0, r=0, t=0, b=100),\n",
    "                legend=dict(\n",
    "                    x=0.5,  # Center the legend\n",
    "                    y=-0.15,  # Position the legend below the x-axis\n",
    "                    xanchor='center',  # Anchor the legend's center at x position\n",
    "                    orientation='h',  # Horizontal orientation of the legend items\n",
    "                ),\n",
    "                font=dict(\n",
    "                    size=14\n",
    "                ),\n",
    "                template='plotly_white',\n",
    "            )\n",
    "\n",
    "    pdf_path = f'figures/culumative.pdf'\n",
    "    fig.write_image(pdf_path)\n",
    "\n",
    "plot_culmulative(oberon_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_latex_parse_table(results_df, include_dist_times=False):\n",
    "    caption = \"Parse times and time to retreive the points-to set for an average method as plotted in Figure~\\\\ref{fig:mean_time} for the projects used in the benchmarks programs.\"\n",
    "    data = [(res['name'], res['parse_mean'], res['means']['mean_time']) for res in results_df]\n",
    "    num_k_values = len(data[0][2])\n",
    "\n",
    "    k_header = \" & \".join([f\"$k$ = {i}\" for i in range(num_k_values)])\n",
    "    print(k_header)\n",
    "    latex_code = [\n",
    "        \"\\\\begin{table}[ht]\",\n",
    "        \"\\\\centering\",\n",
    "        \"\\\\footnotesize\",\n",
    "        \"\\\\begin{tabular}{l\" + (\"c\" * (num_k_values + 1)) + \"}\" if include_dist_times else \"\\\\begin{tabular}{lc}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"\\\\multicolumn{2}{l}{} & \\\\multicolumn{\" + str(num_k_values) + \"}{c}{Mean times for different values of $k$ (s)} \\\\\\\\\" if include_dist_times else \"\",\n",
    "        \"\\\\cmidrule(lr){3-\" + str(2 + num_k_values) + \"}\" if include_dist_times else \"\",\n",
    "        \"Project & Parse time (s)\" + (f\" & {k_header}\" if include_dist_times else \"\") + \" \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ]\n",
    "\n",
    "    for project, parse_time, mean_times in data:\n",
    "        if include_dist_times:\n",
    "            latex_code.append(f\"\\\\tool{{{project}}} & {parse_time:.2f} & {' & '.join(map(lambda x: f'{x:.2f}', mean_times))} \\\\\\\\\")\n",
    "        else:\n",
    "            latex_code.append(f\"\\\\tool{{{project}}} & {parse_time:.2f} \\\\\\\\\")\n",
    "\n",
    "    latex_code.append(\"\\\\bottomrule\")\n",
    "    latex_code.append(\"\\\\end{tabular}\")\n",
    "    latex_code.append(f\"\\\\caption{{{caption}}}\")\n",
    "    latex_code.append(\"\\\\label{tab:parse_times}\")\n",
    "    latex_code.append(\"\\\\end{table}\")\n",
    "\n",
    "    latex_output = \"\\n\".join(latex_code)\n",
    "    \n",
    "    os.makedirs('generated', exist_ok=True)\n",
    "    \n",
    "    with open('generated/parse_table.tex', 'w') as file:\n",
    "        file.write(latex_output)\n",
    "\n",
    "gen_latex_parse_table(oberon_res, include_dist_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import time\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "k_values = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"âˆž\"]\n",
    "recall = [0.00, 0.17, 0.43, 0.71, 0.85, 0.94, 0.95]\n",
    "precision = [1.00, 0.92, 0.90, 0.78, 0.74, 0.67, 0.67]\n",
    "correct_types = [2, 108, 271, 450, 537, 599, 603]\n",
    "overapproximated_typed = [0, 10, 31, 125, 190, 294, 301]\n",
    "times = [0.13, 0.31, 0.41, 0.85, 1.94, 2.64, 2.94]\n",
    "\n",
    "qilin_overapproximated = 489\n",
    "qilin_correct_types = 631\n",
    "qilin_recall = 1.0\n",
    "qilin_precision = 0.56\n",
    "qilin_time = 8.52\n",
    "\n",
    "\n",
    "recall_color = \"#636efb\"\n",
    "precision_color = \"#f0553b\"\n",
    "time_color = \"#00cc96\"\n",
    "culm_color = \"grey\"\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=k_values, y=recall, mode='lines+markers', name='Type Recall', line=dict(dash='dot', color=recall_color)), secondary_y=False)\n",
    "fig.add_trace(go.Scatter(x=k_values, y=precision, mode='lines+markers', name='Type Precision', line=dict(dash='dash', color=precision_color)), secondary_y=False)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=k_values, y=times, mode='lines+markers', name='Time (s)', line=dict(dash='solid', color=time_color)), secondary_y=True)\n",
    "\n",
    "fig.add_hline(y=qilin_recall, line_dash=\"dash\", line_color=recall_color, annotation_text=\"Qilin Recall\", annotation_position=\"top right\", secondary_y=False)\n",
    "fig.add_hline(y=qilin_precision, line_dash=\"dash\", line_color=precision_color, annotation_text=\"Qilin Precision\", annotation_position=\"bottom right\", secondary_y=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"\",\n",
    "    xaxis_title=\"Distance\",\n",
    "    yaxis_title=\"Precision/Recall\",\n",
    "    yaxis2_title=\"Time (s)\",\n",
    "    legend_title=\"Metric\",\n",
    "    template='plotly_white',\n",
    "    margin=dict(l=0, r=0, t=30, b=100),\n",
    "    legend=dict(\n",
    "        x=0.5,\n",
    "        y=-0.2,\n",
    "        xanchor='center',\n",
    "        orientation='h'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_yaxes(range=[0, 1.05], secondary_y=False)\n",
    "fig.update_yaxes(range=[0, max(times) + 0.5], secondary_y=True)\n",
    "\n",
    "fig.write_image(\"figures/qilin_comparison.pdf\")\n",
    "time.sleep(1) # for some reason the first plot generates the box with \"Loading [MathJax]/extensions/MathMenu.js\"\n",
    "fig.write_image(\"figures/qilin_comparison.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python bfpa",
   "language": "python",
   "name": "py-bfpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
